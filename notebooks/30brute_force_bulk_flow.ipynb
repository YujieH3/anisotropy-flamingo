{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 159\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m zmax \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0.03\u001b[39m, np\u001b[38;5;241m.\u001b[39mmax(z_obs)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.05\u001b[39m):\n\u001b[1;32m    158\u001b[0m     zmask \u001b[38;5;241m=\u001b[39m (z_obs \u001b[38;5;241m<\u001b[39m zmax)\n\u001b[0;32m--> 159\u001b[0m     ubf, lon, lat, min_scat, ubf_arr, vlon_arr, vlat_arr, scat_arr \u001b[38;5;241m=\u001b[39m fit_bulk_flow(Y\u001b[38;5;241m=\u001b[39mY[zmask], X\u001b[38;5;241m=\u001b[39mX[zmask], \n\u001b[1;32m    160\u001b[0m                                     z_obs\u001b[38;5;241m=\u001b[39mz_obs[zmask], \n\u001b[1;32m    161\u001b[0m                                     phi_lc\u001b[38;5;241m=\u001b[39mphi_lc[zmask], theta_lc\u001b[38;5;241m=\u001b[39mtheta_lc[zmask],\n\u001b[1;32m    162\u001b[0m                                     yname\u001b[38;5;241m=\u001b[39myname, xname\u001b[38;5;241m=\u001b[39mxname,\n\u001b[1;32m    163\u001b[0m                                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mFIT_RANGE[scaling_relation])\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     plt\u001b[38;5;241m.\u001b[39mscatter(ubf_arr, scat_arr, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 87\u001b[0m, in \u001b[0;36mfit_bulk_flow\u001b[0;34m(Y, X, z_obs, phi_lc, theta_lc, yname, xname, B_min, B_max, scat_min, scat_max, logA_min, logA_max)\u001b[0m\n\u001b[1;32m     85\u001b[0m logY_ \u001b[38;5;241m=\u001b[39m cf\u001b[38;5;241m.\u001b[39mlogY_(Y\u001b[38;5;241m*\u001b[39m(DA_zbf)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m(DA_zobs)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, z\u001b[38;5;241m=\u001b[39mz_bf, relation\u001b[38;5;241m=\u001b[39mscaling_relation)\n\u001b[1;32m     86\u001b[0m logX_ \u001b[38;5;241m=\u001b[39m cf\u001b[38;5;241m.\u001b[39mlogX_(X, relation\u001b[38;5;241m=\u001b[39mscaling_relation)\n\u001b[0;32m---> 87\u001b[0m params \u001b[38;5;241m=\u001b[39m cf\u001b[38;5;241m.\u001b[39mrun_fit(logY_, logX_, scat_step\u001b[38;5;241m=\u001b[39mSCAT_STEP,\n\u001b[1;32m     88\u001b[0m                     B_step\u001b[38;5;241m=\u001b[39mB_STEP, logA_step\u001b[38;5;241m=\u001b[39mLOGA_STEP,\n\u001b[1;32m     89\u001b[0m                     B_min\u001b[38;5;241m=\u001b[39mB_min,\n\u001b[1;32m     90\u001b[0m                     B_max\u001b[38;5;241m=\u001b[39mB_max,\n\u001b[1;32m     91\u001b[0m                     scat_min\u001b[38;5;241m=\u001b[39mscat_min,\n\u001b[1;32m     92\u001b[0m                     scat_max\u001b[38;5;241m=\u001b[39mscat_max,\n\u001b[1;32m     93\u001b[0m                     logA_min\u001b[38;5;241m=\u001b[39mlogA_min,\n\u001b[1;32m     94\u001b[0m                     logA_max\u001b[38;5;241m=\u001b[39mlogA_max,\n\u001b[1;32m     95\u001b[0m                     )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Parallel index. idx += 1 does not work! f u numba.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m idx \u001b[38;5;241m=\u001b[39m (ubf\u001b[38;5;241m-\u001b[39mUBFMIN)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mUBF_STEP \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m360\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mLON_STEP) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m180\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mLAT_STEP) \\\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;241m+\u001b[39m (vlon\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m180\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mLON_STEP \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m180\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mLAT_STEP) \\\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;241m+\u001b[39m (vlat\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m90\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mLAT_STEP\n",
      "File \u001b[0;32m/data1/yujiehe/conda-env/halo/lib/python3.11/site-packages/numba/core/serialize.py:30\u001b[0m, in \u001b[0;36m_numba_unpickle\u001b[0;34m(address, bytedata, hashed)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Keep unpickled object via `numba_unpickle` alive.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m _unpickled_memo \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numba_unpickle\u001b[39m(address, bytedata, hashed):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Used by `numba_unpickle` from _helperlib.c\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m        unpickled object\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     key \u001b[38;5;241m=\u001b[39m (address, hashed)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "For every bulk flow direction and amplitude, calculate the full sample and fit a\n",
    "power law. The one with the least scatter is the best fit bulk flow.\n",
    "\n",
    "quantities affected by bulk flow:\n",
    "- observed redshift -> LX, YSZ, Mgas by bulk flow.\n",
    "but there's no need to start from scratch! Just scale the quantities accodingly\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------IMPORTS------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit, prange, set_num_threads\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data1/yujiehe/anisotropy-flamingo')\n",
    "import tools.constants as const\n",
    "import tools.clusterfit as cf\n",
    "import tools.xray_correct as xc\n",
    "\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "cosmo = FlatLambdaCDM(H0=68.1, Om0=0.306, Ob0=0.0486) # The flamingo fiducial cosmology\n",
    "# import astropy.coordinates as coord\n",
    "# -----------------------CONFIGURATION------------------------------------------\n",
    "\n",
    "# Input file is a halo catalog with lightcone data.\n",
    "INPUT_FILE = '/data1/yujiehe/data/samples_in_lightcone0_with_trees_duplicate_excision_outlier_excision.csv'\n",
    "OUTPUT_FILE = '/data1/yujiehe/data/fits/bulk_flow_lightcone0.csv'\n",
    "OVERWRITE = True\n",
    "\n",
    "# Relations to fit\n",
    "RELATIONS = ['LX-T', 'YSZ-T', 'M-T', 'LX-YSZ', 'LX-M', 'YSZ-M'] # pick from 'LX-T', 'M-T', 'LX-YSZ', 'LX-M', 'YSZ-M', 'YSZ-T'\n",
    "\n",
    "UBFMIN = 0 # ubf for bulk flow velocity\n",
    "UBFMAX = 1000\n",
    "\n",
    "UBF_STEP = 10\n",
    "LON_STEP = 4\n",
    "LAT_STEP = 2\n",
    "\n",
    "# Nnumber of threads\n",
    "N_THREADS = 8\n",
    "\n",
    "B_STEP       = 0.003\n",
    "LOGA_STEP    = 0.003\n",
    "SCAT_STEP    = 0.003\n",
    "\n",
    "C = 299792.458                  # the speed of light in km/s\n",
    "FIT_RANGE = const.FIVE_MAX_RANGE_TIGHT_SCAT\n",
    "\n",
    "# -----------------------END CONFIGURATION--------------------------------------\n",
    "\n",
    "\n",
    "#@njit(parallel=True, fastmath=True)\n",
    "def fit_bulk_flow(Y, X, z_obs, phi_lc, theta_lc, yname, xname,\n",
    "                  B_min, B_max, scat_min, scat_max, logA_min, logA_max\n",
    "                  ):\n",
    "    scaling_relation = f'{yname}-{xname}'\n",
    "    min_scat = 1000 # initialize a large number\n",
    "\n",
    "    # Loop over the bulk flow direction and amplitude\n",
    "    n_steps = (UBFMAX - UBFMIN)//UBF_STEP * (360//LON_STEP) * (180//LAT_STEP)\n",
    "    ubf_arr = np.empty(n_steps, dtype=np.float64)\n",
    "    vlon_arr = np.empty(n_steps, dtype=np.float64)\n",
    "    vlat_arr = np.empty(n_steps, dtype=np.float64)\n",
    "    scat_arr = np.empty(n_steps, dtype=np.float64)\n",
    "\n",
    "    idx = 0\n",
    "    for ubf in prange(UBFMIN//UBF_STEP, UBFMAX//UBF_STEP):\n",
    "        ubf = ubf * UBF_STEP + UBFMIN\n",
    "        for vlon in range(-180, 180, LON_STEP):\n",
    "            for vlat in range(-90, 90, LAT_STEP):\n",
    "                # Calculate the redshift\n",
    "                angle = cf.angular_separation(phi_lc, theta_lc, vlon, vlat)\n",
    "\n",
    "                # From: z_bf = z_obs - ubf * (1 + z_bf) * np.cos(angle) / C\n",
    "                z_bf = (z_obs + ubf * np.cos(angle) / C) / (1 - ubf * np.cos(angle) / C) # the ubf convention than the paper\n",
    "                \n",
    "                # Calculate the angular diameter distance\n",
    "                DA_zobs = cosmo.angular_diameter_distance(z_obs).value\n",
    "                DA_zbf = cosmo.angular_diameter_distance(z_bf).value\n",
    "\n",
    "                # To our fit parameters\n",
    "                logY_ = cf.logY_(Y*(DA_zbf)**2/(DA_zobs)**2, z=z_bf, relation=scaling_relation)\n",
    "                logX_ = cf.logX_(X, relation=scaling_relation)\n",
    "                params = cf.run_fit(logY_, logX_, scat_step=SCAT_STEP,\n",
    "                                    B_step=B_STEP, logA_step=LOGA_STEP,\n",
    "                                    B_min=B_min,\n",
    "                                    B_max=B_max,\n",
    "                                    scat_min=scat_min,\n",
    "                                    scat_max=scat_max,\n",
    "                                    logA_min=logA_min,\n",
    "                                    logA_max=logA_max,\n",
    "                                    )\n",
    "\n",
    "                # Parallel index. idx += 1 does not work! f u numba.\n",
    "                idx = (ubf-UBFMIN)//UBF_STEP * (360//LON_STEP) * (180//LAT_STEP) \\\n",
    "                    + (vlon+180)//LON_STEP * (180//LAT_STEP) \\\n",
    "                    + (vlat+90)//LAT_STEP\n",
    "\n",
    "                ubf_arr[idx] = ubf\n",
    "                vlon_arr[idx] = vlon\n",
    "                vlat_arr[idx] = vlat\n",
    "                scat_arr[idx] = params['scat']\n",
    "\n",
    "                # numba prange parallel loops can infer automatically for +=, *=, -=, /= \n",
    "                # idx += 1 \n",
    "                # it doesn not work lol\n",
    "    # print(scat_arr, np.min(scat_arr), np.max(scat_arr))\n",
    "\n",
    "    # The best fit index\n",
    "    fit_idx = np.argmin(scat_arr)\n",
    "\n",
    "    # Save the best fit parameters\n",
    "    fit_ubf = ubf_arr[fit_idx]\n",
    "    fit_vlon = vlon_arr[fit_idx]\n",
    "    fit_vlat = vlat_arr[fit_idx]\n",
    "    min_scat = scat_arr[fit_idx]\n",
    "\n",
    "    # For the sake of debugging also save all the fit parameters\n",
    "\n",
    "    return fit_ubf, fit_vlon, fit_vlat, min_scat, \\\n",
    "        ubf_arr, vlon_arr, vlat_arr, scat_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Set the number of threads\n",
    "set_num_threads(N_THREADS)\n",
    "\n",
    "\n",
    "# Load the sample\n",
    "halo_data = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "for scaling_relation in RELATIONS:\n",
    "\n",
    "    n_clusters = cf.CONST[scaling_relation]['N']\n",
    "\n",
    "    _ = scaling_relation.find('-')\n",
    "    yname = scaling_relation[:_]\n",
    "    xname = scaling_relation[_+1:]\n",
    "    Y = np.array(halo_data[cf.COLUMNS[yname]][:n_clusters])\n",
    "    X = np.array(halo_data[cf.COLUMNS[xname]][:n_clusters])\n",
    "\n",
    "    # Also load the position data\n",
    "    phi_lc   = np.array(halo_data['phi_on_lc'][:n_clusters])\n",
    "    theta_lc = np.array(halo_data['theta_on_lc'][:n_clusters])\n",
    "        \n",
    "    # the cosmological redshift from lightcone (no peculiar velocity attached)\n",
    "    z_obs = np.array(halo_data['ObservedRedshift'][:n_clusters])\n",
    "\n",
    "    for zmax in np.arange(0.03, np.max(z_obs)+0.01, 0.05):\n",
    "        zmask = (z_obs < zmax)\n",
    "        ubf, lon, lat, min_scat, ubf_arr, vlon_arr, vlat_arr, scat_arr = fit_bulk_flow(Y=Y[zmask], X=X[zmask], \n",
    "                                        z_obs=z_obs[zmask], \n",
    "                                        phi_lc=phi_lc[zmask], theta_lc=theta_lc[zmask],\n",
    "                                        yname=yname, xname=xname,\n",
    "                                        **FIT_RANGE[scaling_relation])\n",
    "        \n",
    "        # Plot the results\n",
    "        plt.scatter(ubf_arr, scat_arr, s=1)\n",
    "        plt.axhline(min_scat, color='red')\n",
    "        plt.axvline(ubf, color='red')\n",
    "        plt.xlabel('ubf')\n",
    "        plt.ylabel('scat')\n",
    "        break\n",
    "    \n",
    "    break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 80.0, 30.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ubf, lon, lat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
