{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H0 anisotropy\n",
    "## 1. With propagated error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H0_dipole_variation_propagated(scan_bf, scan_bt, n, A_all, A_all_plus, A_all_minus):\n",
    "    \"\"\"\n",
    "    Calculate the dipole variation with propagated error\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    scan_bf : pandas.DataFrame\n",
    "        Best fit values of the scan.\n",
    "    scan_bt : pandas.DataFrame\n",
    "        Bootstrap values of the scan.\n",
    "    n : float\n",
    "        H0/H0_all = (A/A_all)^n.\n",
    "    A_all : float\n",
    "        Best fit value of A for all clusters.\n",
    "    A_all_plus : float\n",
    "        The positive 1-sigma uncertainty of A_all.\n",
    "    A_all_minus : float\n",
    "        The negative 1-sigma uncertainty of A_all. Here input as a positive number.\n",
    "    \"\"\"\n",
    "    assert A_all_minus > 0\n",
    "    assert A_all_plus > 0\n",
    "\n",
    "    # Use the number of significance so they are automatically the dipole direction\n",
    "    maxidx = scan_bf['n_sigma'].idxmax()\n",
    "    minidx = scan_bf['n_sigma'].idxmin()\n",
    "\n",
    "    A_all_sigma = np.max([A_all_plus, A_all_minus])\n",
    "\n",
    "    # The + region\n",
    "    Glon = scan_bf.loc[maxidx, 'Glon']\n",
    "    Glat = scan_bf.loc[maxidx, 'Glat']\n",
    "\n",
    "    A_max = scan_bf.loc[maxidx, 'A']\n",
    "    A_max_distr = scan_bt[(scan_bt['Glon']==Glon) & (scan_bt['Glat']==Glat)]['A']\n",
    "    A_max_distr = np.array(A_max_distr)\n",
    "\n",
    "    # A_max_plus = np.percentile(A_max_distr, 84) - A_max\n",
    "    # A_max_minus = A_max - np.percentile(A_max_distr, 16)\n",
    "    # A_max_sigma = np.max([A_max_plus, A_max_minus])\n",
    "\n",
    "    A_max_sigma = np.std(A_max_distr) # Use the standard deviation\n",
    "\n",
    "\n",
    "    f_max = best_fit_H0_H0all_max = (A_max/A_all)**n\n",
    "    H0_H0all_sigma_max = np.sqrt(n**2 * f_max**(2*n-2) / A_all**2 * A_max_sigma**2\n",
    "                              + n**2 * f_max**(2*n-2) * A_max**2 / A_all**4 * A_all_sigma**2)\n",
    "    \n",
    "    # The - region\n",
    "    Glon = scan_bf.loc[minidx, 'Glon']\n",
    "    Glat = scan_bf.loc[minidx, 'Glat']\n",
    "\n",
    "    A_min = scan_bf.loc[minidx, 'A']\n",
    "    A_min_distr = scan_bt[(scan_bt['Glon']==Glon) & (scan_bt['Glat']==Glat)]['A']\n",
    "    A_min_distr = np.array(A_min_distr)\n",
    "\n",
    "    # A_min_plus = np.percentile(A_min_distr, 84) - A_min\n",
    "    # A_min_minus = A_min - np.percentile(A_min_distr, 16)\n",
    "    # A_min_sigma = np.max([A_min_plus, A_min_minus])\n",
    "\n",
    "    A_min_sigma = np.std(A_min_distr) # Use the standard deviation\n",
    "\n",
    "    f_min = best_fit_H0_H0all_min = (A_min/A_all)**n\n",
    "    H0_H0all_sigma_min = np.sqrt(n**2 * f_min**(2*n-2) / A_all**2 * A_min_sigma**2\n",
    "                              + n**2 * f_min**(2*n-2) * A_min**2 / A_all**4 * A_all_sigma**2)\n",
    "    \n",
    "    H0_H0all_sigma = np.sqrt(H0_H0all_sigma_max**2 + H0_H0all_sigma_min**2)\n",
    "    return f_max-f_min, H0_H0all_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LX-T & 0.045\\pm0.025\n",
      "YSZ-T & 0.091\\pm0.031\n",
      "M-T & 0.022\\pm0.010\n",
      "LX-T & 0.052\\pm0.021\n",
      "YSZ-T & 0.038\\pm0.022\n",
      "M-T & 0.014\\pm0.010\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "n = [1/2, 1/2, 2/5]\n",
    "cone_size = [75, 60, 75]\n",
    "\n",
    "for lc in range(2):\n",
    "    df = pd.read_csv(f'../scripts/fit-all-lightcone{lc}.csv')\n",
    "    for i, relation in enumerate(['LX-T', 'YSZ-T', 'M-T']):\n",
    "        # extract A_all, A_all_lower, A_all_upper\n",
    "        mask = df['relation'] == relation\n",
    "        A_all = df[mask]['A'].values[0]\n",
    "        A_all_lower = df[mask]['A_lower'].values[0]\n",
    "        A_all_upper = df[mask]['A_upper'].values[0]\n",
    "        \n",
    "        scan_bf = pd.read_csv(f'../data/fits/testrun/lightcone{lc}/scan_best_fit_{relation}_θ{cone_size[i]}.csv')\n",
    "        scan_bt = pd.read_csv(f'../data/fits/testrun/lightcone{lc}/scan_bootstrap_{relation}_θ{cone_size[i]}.csv')\n",
    "\n",
    "        # Propagated error\n",
    "        H0_variation, H0_variation_err = H0_dipole_variation_propagated(scan_bf, scan_bt, n[i], \n",
    "                A_all=A_all, A_all_plus=A_all_upper, A_all_minus=A_all_lower)\n",
    "\n",
    "        print(relation, '&', f'{H0_variation:.3f}\\\\pm{H0_variation_err:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using number of significance directly, as Kostas suggested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H0_dipole_variation_nosigma(scan_bf, n, A_all):\n",
    "    \"\"\"\n",
    "    Calculate the dipole variation without propagated error\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    scan_bf : pandas.DataFrame\n",
    "        Best fit values of the scan.\n",
    "    scan_bt : pandas.DataFrame\n",
    "        Bootstrap values of the scan.\n",
    "    n : float\n",
    "        H0/H0_all = (A/A_all)^n.\n",
    "    A_all : float\n",
    "        Best fit value of A for all clusters.\n",
    "    \"\"\"\n",
    "    # Use the number of significance so they are automatically the dipole direction\n",
    "    maxidx = scan_bf['n_sigma'].idxmax()\n",
    "    # print(maxidx)\n",
    "    minidx = scan_bf['n_sigma'].idxmin()\n",
    "    # print(minidx)\n",
    "\n",
    "    # Max number of significance to calculate the significance of H0 dipole variation\n",
    "    maxnosigma = scan_bf.loc[maxidx, 'n_sigma']\n",
    "    # print(maxnosigma)\n",
    "    minnosigma = scan_bf.loc[minidx, 'n_sigma']\n",
    "    assert maxnosigma + minnosigma == 0 # A sanity check, if they are truely the dipole direction\n",
    "\n",
    "    # The + region\n",
    "    max_Glon = scan_bf.loc[maxidx, 'Glon']\n",
    "    max_Glat = scan_bf.loc[maxidx, 'Glat']\n",
    "\n",
    "    A_max = scan_bf.loc[maxidx, 'A']\n",
    "\n",
    "    f_max = (A_max/A_all)**n\n",
    "    \n",
    "    # The - region\n",
    "    min_Glon = scan_bf.loc[minidx, 'Glon']\n",
    "    min_Glat = scan_bf.loc[minidx, 'Glat']\n",
    "\n",
    "    A_min = scan_bf.loc[minidx, 'A']\n",
    "\n",
    "    f_min = (A_min/A_all)**n\n",
    " \n",
    "    H0_variation = f_max-f_min\n",
    "    H0_variation_err = H0_variation / maxnosigma\n",
    "\n",
    "    return max_Glon, max_Glat, H0_variation, H0_variation_err, maxnosigma \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\midrule\n",
      "\\multicolumn{4}{c}{Lightcone 1} \\\\\n",
      "\\midrule\n",
      "\\LXT & $(\\ang{-144},\\ang{8})$ & $4.47\\pm2.05\\%$ & $2.19\\sigma$ \\\\\n",
      "\\YSZT & $(\\ang{-68},\\ang{20})$ & $9.07\\pm2.45\\%$ & $3.70\\sigma$ \\\\\n",
      "\\MgasT & $(\\ang{-84},\\ang{12})$ & $2.22\\pm0.72\\%$ & $3.07\\sigma$ \\\\\n",
      "Combined & $(\\ang{-68},\\ang{20})$ & $3.15\\pm0.78\\%$ & $4.02\\sigma$ \\\\\n",
      "\\midrule\n",
      "\\multicolumn{4}{c}{Lightcone 2} \\\\\n",
      "\\midrule\n",
      "\\LXT & $(\\ang{128},\\ang{-22})$ & $5.18\\pm1.61\\%$ & $3.21\\sigma$ \\\\\n",
      "\\YSZT & $(\\ang{128},\\ang{-4})$ & $3.78\\pm1.54\\%$ & $2.46\\sigma$ \\\\\n",
      "\\MgasT & $(\\ang{124},\\ang{-16})$ & $1.38\\pm0.64\\%$ & $2.17\\sigma$ \\\\\n",
      "Combined & $(\\ang{128},\\ang{-16})$ & $2.50\\pm0.71\\%$ & $3.55\\sigma$ \\\\\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../tools/')\n",
    "import clusterfit as cf\n",
    "\n",
    "n = [1/2, 1/2, 2/5]\n",
    "cone_size = [75, 60, 75]\n",
    "label = ['\\\\LXT', '\\\\YSZT', '\\\\MgasT']\n",
    "H0_all = 68.1\n",
    "\n",
    "for lc in range(2):\n",
    "    df = pd.read_csv(f'../scripts/fit-all-lightcone{lc}.csv')\n",
    "    print('\\\\midrule')\n",
    "    print(f'\\\\multicolumn{{4}}{{c}}{{Lightcone {lc+1}}} \\\\\\\\')\n",
    "    print('\\\\midrule')\n",
    "\n",
    "\n",
    "    # Initialize the arrays. It works only with 90*90 resolution but what the fuck\n",
    "    H0_var_sigma_maps = np.empty((3, 8100)) # Uncertainty of the DIPOLE variation in %, should be symmetrized by nature\n",
    "    H0_var_maps       = np.empty((3, 8100)) # H0 variation maps in % \n",
    "    n_sigma_maps      = np.empty((3, 8100)) # Number of sigma of the dipole variation\n",
    "\n",
    "    for i, relation in enumerate(['LX-T', 'YSZ-T', 'M-T']):\n",
    "        # extract A_all, A_all_lower, A_all_upper\n",
    "        mask = df['relation'] == relation\n",
    "        A_all = df[mask]['A'].values[0]\n",
    "        \n",
    "        scan_bf = pd.read_csv(f'../data/fits/testrun/lightcone{lc}/scan_best_fit_{relation}_θ{cone_size[i]}.csv')\n",
    "        # scan_bt = pd.read_csv(f'../data/fits/testrun/lightcone{lc}/scan_bootstrap_{relation}_θ{cone_size[i]}.csv')\n",
    "\n",
    "        # Error using number of sigma\n",
    "        lon, lat, H0_variation, H0_variation_err, max_no_sigma = H0_dipole_variation_nosigma(scan_bf, n[i], A_all=A_all)\n",
    "\n",
    "        # In latex format for the table\n",
    "        print(label[i], '&', f'$(\\\\ang{{{lon:.0f}}},\\\\ang{{{lat:.0f}}})$', '&', f'${H0_variation*100:.2f}\\\\pm{H0_variation_err*100:.2f}\\\\%$', '&', f'${max_no_sigma:.2f}\\\\sigma$ \\\\\\\\')\n",
    "\n",
    "        # Calculate the H0 variation of the combined map\n",
    "        best_fit_file = f'../data/fits/testrun/lightcone{lc}/scan_best_fit_{relation}_θ{cone_size[i]}.csv'\n",
    "        best_fit = pd.read_csv(best_fit_file)\n",
    "\n",
    "        # Load the A map\n",
    "        A_map = best_fit['A'].values\n",
    "        lons = best_fit['Glon'].values\n",
    "        lats = best_fit['Glat'].values\n",
    "        n_sigma_map = best_fit['n_sigma'].values\n",
    "\n",
    "        # Replace the zeros with a small number\n",
    "        n_sigma_map[n_sigma_map == 0] = 1e-5\n",
    "\n",
    "        # Save to number of sigma maps\n",
    "        n_sigma_maps[i] = n_sigma_map\n",
    "\n",
    "        # Calculate the H0 map and symmetrize to obtain the dipole map\n",
    "        H0_var_maps[i] = (A_map/A_all)**n[i] - 1\n",
    "        H0_var_maps[i] = cf.make_dipole_map(H0_var_maps[i], lons, lats, central=0)\n",
    "        # print(cf.find_dipole_in_dipole_map(H0_maps[i], lons, lats))\n",
    "        \n",
    "        # H0 error map. Both of the maps are symmetrized so H0_sigma_maps are also symmetrized\n",
    "        H0_var_sigma_maps[i] = H0_var_maps[i] / n_sigma_maps[i]\n",
    "        H0_var_sigma_maps[i][H0_var_sigma_maps[i] == 0] = 1e5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Combine the variation maps\n",
    "    joint_H0_var_map = np.average(H0_var_maps, axis=0, weights=1/H0_var_sigma_maps**2)\n",
    "\n",
    "    # Combine the number of significance (in a WRONG way for the thesis only)\n",
    "    joint_sigma_map = np.sqrt(1/np.sum(1/H0_var_sigma_maps**2, axis=0))\n",
    "    joint_n_sigma_map = joint_H0_var_map / joint_sigma_map # both of them are symmetrized, so joint_n_sigma_map is also symmetrized\n",
    "\n",
    "    # Find the max dipole as highest significance\n",
    "    max_n_sigma, min_n_sigma, lon, lat, maxloc = cf.find_dipole_in_dipole_map(joint_n_sigma_map, lons, lats)\n",
    "\n",
    "    # Calculate the H0 variation of the combined map\n",
    "    max_H0_var = joint_H0_var_map[maxloc] * 2 # The dipole is symmetrized, so we need to multiply by 2\n",
    "    # joint_H0_map = joint_H0_map.reshape(90, 90).T # If you need to visualize\n",
    "\n",
    "    max_var_err = max_H0_var / max_n_sigma\n",
    "\n",
    "    print('Combined', '&', f'$(\\\\ang{{{lon:.0f}}},\\\\ang{{{lat:.0f}}})$', '&', f'${max_H0_var*100:.2f}\\\\pm{max_var_err*100:.2f}\\\\%$', '&', f'${max_n_sigma:.2f}\\\\sigma$ \\\\\\\\') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Mean: 3.1494382022471914\n",
      "Uncertainty of the Weighted Mean: 0.7631984736045793\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Given values and uncertainties\n",
    "values = np.array([2.4, 9.1, 2.5])\n",
    "uncertainties = np.array([1.8, 2.4, 0.9])\n",
    "# Calculate weights\n",
    "weights = 1 / uncertainties**2\n",
    "# Calculate the weighted mean\n",
    "weighted_mean = np.sum(weights * values) / np.sum(weights)\n",
    "# Calculate the uncertainty of the weighted mean\n",
    "weighted_mean_uncertainty = np.sqrt(1 / np.sum(weights))\n",
    "weighted_mean, weighted_mean_uncertainty\n",
    "print(\"Weighted Mean:\", weighted_mean)\n",
    "print(\"Uncertainty of the Weighted Mean:\", weighted_mean_uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2575\n",
      "0.02425908626760498\n",
      "0.018231384594117588\n",
      "0.0906776782566856\n",
      "0.024539030354470262\n",
      "0.024867678056429532\n",
      "0.009266344056124696\n"
     ]
    }
   ],
   "source": [
    "print(maxloc)\n",
    "for i in range(len(H0_var_maps)):\n",
    "    print((H0_var_maps[i,maxloc])*2)\n",
    "    print((H0_var_sigma_maps[i,maxloc])*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we combine only LX and YSZ? considering the correlation between relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\midrule\n",
      "\\multicolumn{4}{c}{Lightcone 1} \\\\\n",
      "\\midrule\n",
      "\\LXT & $(\\ang{-144},\\ang{8})$ & $4.47\\pm2.05\\%$ & $2.19\\sigma$ \\\\\n",
      "\\YSZT & $(\\ang{-68},\\ang{20})$ & $9.07\\pm2.45\\%$ & $3.70\\sigma$ \\\\\n",
      "Combined & $(\\ang{-68},\\ang{22})$ & $5.20\\pm1.52\\%$ & $3.42\\sigma$ \\\\\n",
      "\\midrule\n",
      "\\multicolumn{4}{c}{Lightcone 2} \\\\\n",
      "\\midrule\n",
      "\\LXT & $(\\ang{128},\\ang{-22})$ & $5.18\\pm1.61\\%$ & $3.21\\sigma$ \\\\\n",
      "\\YSZT & $(\\ang{128},\\ang{-4})$ & $3.78\\pm1.54\\%$ & $2.46\\sigma$ \\\\\n",
      "Combined & $(\\ang{136},\\ang{-16})$ & $4.45\\pm1.21\\%$ & $3.67\\sigma$ \\\\\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../tools/')\n",
    "import clusterfit as cf\n",
    "\n",
    "n = [1/2, 1/2, 2/5]\n",
    "cone_size = [75, 60, 75]\n",
    "label = ['\\\\LXT', '\\\\YSZT', '\\\\MgasT']\n",
    "H0_all = 68.1\n",
    "\n",
    "for lc in range(2):\n",
    "    df = pd.read_csv(f'../scripts/fit-all-lightcone{lc}.csv')\n",
    "    print('\\\\midrule')\n",
    "    print(f'\\\\multicolumn{{4}}{{c}}{{Lightcone {lc+1}}} \\\\\\\\')\n",
    "    print('\\\\midrule')\n",
    "\n",
    "\n",
    "    # Initialize the arrays. It works only with 90*90 resolution but what the fuck\n",
    "    H0_var_sigma_maps = np.empty((2, 8100)) # Uncertainty of the DIPOLE variation in %, should be symmetrized by nature\n",
    "    H0_var_maps       = np.empty((2, 8100)) # H0 variation maps in % \n",
    "    n_sigma_maps      = np.empty((2, 8100)) # Number of sigma of the dipole variation\n",
    "\n",
    "    for i, relation in enumerate(['LX-T', 'YSZ-T']):\n",
    "        # extract A_all, A_all_lower, A_all_upper\n",
    "        mask = df['relation'] == relation\n",
    "        A_all = df[mask]['A'].values[0]\n",
    "        \n",
    "        scan_bf = pd.read_csv(f'../data/fits/testrun/lightcone{lc}/scan_best_fit_{relation}_θ{cone_size[i]}.csv')\n",
    "        # scan_bt = pd.read_csv(f'../data/fits/testrun/lightcone{lc}/scan_bootstrap_{relation}_θ{cone_size[i]}.csv')\n",
    "\n",
    "        # Error using number of sigma\n",
    "        lon, lat, H0_variation, H0_variation_err, max_no_sigma = H0_dipole_variation_nosigma(scan_bf, n[i], A_all=A_all)\n",
    "\n",
    "        # In latex format for the table\n",
    "        print(label[i], '&', f'$(\\\\ang{{{lon:.0f}}},\\\\ang{{{lat:.0f}}})$', '&', f'${H0_variation*100:.2f}\\\\pm{H0_variation_err*100:.2f}\\\\%$', '&', f'${max_no_sigma:.2f}\\\\sigma$ \\\\\\\\')\n",
    "\n",
    "        # Calculate the H0 variation of the combined map\n",
    "        best_fit_file = f'../data/fits/testrun/lightcone{lc}/scan_best_fit_{relation}_θ{cone_size[i]}.csv'\n",
    "        best_fit = pd.read_csv(best_fit_file)\n",
    "\n",
    "        # Load the A map\n",
    "        A_map = best_fit['A'].values\n",
    "        lons = best_fit['Glon'].values\n",
    "        lats = best_fit['Glat'].values\n",
    "        n_sigma_map = best_fit['n_sigma'].values\n",
    "\n",
    "        # Replace the zeros with a small number\n",
    "        n_sigma_map[n_sigma_map == 0] = 1e-5\n",
    "\n",
    "        # Save to number of sigma maps\n",
    "        n_sigma_maps[i] = n_sigma_map\n",
    "\n",
    "        # Calculate the H0 map and symmetrize to obtain the dipole map\n",
    "        H0_var_maps[i] = (A_map/A_all)**n[i] - 1\n",
    "        H0_var_maps[i] = cf.make_dipole_map(H0_var_maps[i], lons, lats, central=0)\n",
    "        # print(cf.find_dipole_in_dipole_map(H0_maps[i], lons, lats))\n",
    "        \n",
    "        # H0 error map. Both of the maps are symmetrized so H0_sigma_maps are also symmetrized\n",
    "        H0_var_sigma_maps[i] = H0_var_maps[i] / n_sigma_maps[i]\n",
    "        H0_var_sigma_maps[i][H0_var_sigma_maps[i] == 0] = 1e5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Combine the variation maps\n",
    "    joint_H0_var_map = np.average(H0_var_maps, axis=0, weights=1/H0_var_sigma_maps**2)\n",
    "\n",
    "    # Combine the number of significance (in a WRONG way for the thesis only)\n",
    "    joint_sigma_map = np.sqrt(1/np.sum(1/H0_var_sigma_maps**2, axis=0))\n",
    "    joint_n_sigma_map = joint_H0_var_map / joint_sigma_map # both of them are symmetrized, so joint_n_sigma_map is also symmetrized\n",
    "\n",
    "    # Find the max dipole as highest significance\n",
    "    max_n_sigma, min_n_sigma, lon, lat, maxloc = cf.find_dipole_in_dipole_map(joint_n_sigma_map, lons, lats)\n",
    "\n",
    "    # Calculate the H0 variation of the combined map\n",
    "    max_H0_var = joint_H0_var_map[maxloc]*2\n",
    "    # joint_H0_map = joint_H0_map.reshape(90, 90).T # If you need to visualize\n",
    "\n",
    "    max_var_err = max_H0_var / max_n_sigma\n",
    "\n",
    "    print('Combined', '&', f'$(\\\\ang{{{lon:.0f}}},\\\\ang{{{lat:.0f}}})$', '&', f'${max_H0_var*100:.2f}\\\\pm{max_var_err*100:.2f}\\\\%$', '&', f'${max_n_sigma:.2f}\\\\sigma$ \\\\\\\\') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
